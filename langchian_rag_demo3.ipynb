{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-07T05:03:54.487709Z",
     "start_time": "2026-01-07T05:03:46.618339Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_classic.output_parsers import ResponseSchema\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "# ChatPromptTemplate\n",
    "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "\n",
    "# basic_qa_chain = model | StrOutputParser()\n",
    "\n",
    "# response = basic_qa_chain.invoke(\"è¯·è‡ªæˆ‘ä»‹ç»ã€‚\")\n",
    "# print(response)\n",
    "\n",
    "# è§£æå¸ƒå°”è¾“å‡º\n",
    "from langchain_classic.output_parsers.boolean import BooleanOutputParser\n",
    "# BooleanOutputParser\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„å¸®æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºå›ç­”ã€‚\"),\n",
    "    (\"user\", \"è¿™æ˜¯ç”¨æˆ·çš„é—®é¢˜ï¼š{topic}\")\n",
    "])\n",
    "\n",
    "math_qa_chain = prompt_template | model | StrOutputParser()\n",
    "\n",
    "question1 = \\\n",
    "\"\"\"\n",
    "Alice and Bob are each holding some integer number of sweets. Alice says to Bob: â€œIf\n",
    "we each added the number of sweets weâ€™re holding to our (positive integer) age, my answer would\n",
    "be double yours. If we took the product, then my answer would be four times yours.â€ Bob replies:\n",
    "â€œWhy donâ€™t you give me five of your sweets because then both our sum and product would be equal.â€\n",
    "What is the product of Alice and Bobâ€™s ages?\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "question = \\\n",
    "\"\"\"\n",
    "what is 1 / 0?\n",
    "\"\"\"\n",
    "\n",
    "result = math_qa_chain.invoke({\"topic\": question})\n",
    "\n",
    "print(result)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ•°å­¦é—®é¢˜ï¼  \n",
      "\n",
      "1 é™¤ä»¥ 0 åœ¨æ•°å­¦ä¸­**æ²¡æœ‰å®šä¹‰**ï¼Œä¹Ÿå°±æ˜¯è¯´å®ƒæ²¡æœ‰æ„ä¹‰ã€‚åŸå› å¦‚ä¸‹ï¼š  \n",
      "\n",
      "- é™¤æ³•å¯ä»¥ç†è§£ä¸ºâ€œä¸€ä¸ªæ•°ä¹˜ä»¥å¤šå°‘ç­‰äºå¦ä¸€ä¸ªæ•°â€ã€‚  \n",
      "  æ¯”å¦‚ \\( 1 / 0 = x \\) æ„å‘³ç€ \\( 0 \\times x = 1 \\)ï¼Œä½†ä»»ä½•æ•°ä¹˜ä»¥ 0 éƒ½ç­‰äº 0ï¼Œä¸å¯èƒ½ç­‰äº 1ï¼Œæ‰€ä»¥è¿™æ ·çš„ \\( x \\) ä¸å­˜åœ¨ã€‚  \n",
      "\n",
      "- ä»æé™çš„è§’åº¦çœ‹ï¼Œå¦‚æœè®©é™¤æ•°è¶Šæ¥è¶Šæ¥è¿‘ 0ï¼Œæ¯”å¦‚ \\( 1 / 0.1 = 10 \\)ï¼Œ\\( 1 / 0.01 = 100 \\)ï¼Œ\\( 1 / 0.001 = 1000 \\)â€¦â€¦ç»“æœä¼šè¶‹å‘æ— ç©·å¤§ï¼Œä½†æ— ç©·å¤§ä¸æ˜¯ä¸€ä¸ªç¡®å®šçš„å®æ•°ï¼Œè€Œä¸”ä»è´Ÿæ–¹å‘æ¥è¿‘ 0 ä¼šè¶‹å‘è´Ÿæ— ç©·ï¼Œæ‰€ä»¥æé™ä¹Ÿä¸å”¯ä¸€ã€‚  \n",
      "\n",
      "å› æ­¤ï¼Œåœ¨æ ‡å‡†ç®—æœ¯ä¸­ï¼Œ**é™¤ä»¥é›¶æ˜¯æœªå®šä¹‰çš„**ã€‚\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-08T09:36:45.567116Z",
     "start_time": "2026-01-08T09:36:43.805431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_classic.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "\n",
    "schemas = [\n",
    "    ResponseSchema(name=\"name\", description=\"ç”¨æˆ·çš„å§“å\"),\n",
    "    ResponseSchema(name=\"age\", description=\"ç”¨æˆ·çš„å¹´é¾„\")\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(schemas)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"è¯·æ ¹æ®ä»¥ä¸‹å†…å®¹æå–ç”¨æˆ·ä¿¡æ¯ï¼Œå¹¶è¿”å› JSON æ ¼å¼ï¼š\\n{input}\\n\\n{format_instructions}\"\n",
    ")\n",
    "\n",
    "format_instructions = parser.get_format_instructions()\n",
    "print(format_instructions)\n",
    "\n",
    "chain = (\n",
    "    prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"input\": \"ç”¨æˆ·å«æé›·ï¼Œä»Šå¹´25å²ï¼Œæ˜¯ä¸€åå·¥ç¨‹å¸ˆã€‚\"})\n",
    "\n",
    "print(result)"
   ],
   "id": "cc74a47d65491b18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"name\": string  // ç”¨æˆ·çš„å§“å\n",
      "\t\"age\": string  // ç”¨æˆ·çš„å¹´é¾„\n",
      "}\n",
      "```\n",
      "{'name': 'æé›·', 'age': '25'}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "LECL -- LangChainExpressionLanguage\n",
    "å‡½æ•°å¼ç®¡é“é£æ ¼ï¼Œå…è®¸é€šè¿‡ç®¡é“ç¬¦è¿æ¥ä¸¤ä¸ªå…ƒç´ ï¼Œå®ç°åŠŸèƒ½çš„ä¸²è”\n",
    "invoke\n",
    "stream\n",
    "batch"
   ],
   "id": "1b807ae0c57b7dd8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T09:16:37.874807Z",
     "start_time": "2026-01-09T09:16:26.338791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# æ„å»ºå¤åˆé“¾\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain_classic.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "\n",
    "news_gen_prompt = PromptTemplate.from_template(\n",
    "    \"è¯·æ ¹æ®ä»¥ä¸‹æ–°é—»æ ‡é¢˜æ’°å†™ä¸€åˆ™ç®€çŸ­çš„æ–°é—»å†…å®¹ï¼Œ100å­—ä»¥å†…ï¼š\\n\\n{title}\"\n",
    ")\n",
    "\n",
    "# modelè¾“å‡ºä¸€ä¸ªAIMessage\n",
    "news_chain = news_gen_prompt | model\n",
    "\n",
    "schemas = [\n",
    "    ResponseSchema(name=\"time\", description=\"äº‹ä»¶å‘ç”Ÿçš„æ—¶é—´\"),\n",
    "    ResponseSchema(name=\"location\", description=\"äº‹ä»¶å‘ç”Ÿçš„åœ°ç‚¹\"),\n",
    "    ResponseSchema(name=\"event\", description=\"å‘ç”Ÿçš„å…·ä½“äº‹ä»¶\")\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(schemas)\n",
    "\n",
    "summary_prompt = PromptTemplate.from_template(\n",
    "    \"è¯·ä»ä¸‹é¢çš„æ–°é—»å†…å®¹ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œå¹¶è¿”å›ç»“æ„åŒ–çš„jsonæ ¼å¼ï¼š\\n\\n{news}\\n\\n{format_instructions}\"\n",
    ")\n",
    "\n",
    "summary_chain = (\n",
    "    summary_prompt.partial(format_instructions=parser.get_format_instructions())\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "combined_chain = news_chain | summary_chain\n",
    "\n",
    "response = combined_chain.invoke({\"title\": \"é™¨çŸ³æ’å‡»åœ°çƒ\"})\n",
    "\n",
    "print(response)\n"
   ],
   "id": "428d52718a9f2a3a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 'æ˜¨æ—¥å‡Œæ™¨', 'location': 'å¤ªå¹³æ´‹æ— äººæµ·åŸŸ', 'event': 'ä¸€é¢—ç›´å¾„çº¦10ç±³çš„é™¨çŸ³å å…¥ï¼Œæœªé€ æˆäººå‘˜ä¼¤äº¡åŠè´¢äº§æŸå¤±ï¼Œç›¸å…³æœºæ„æ­£å¯¹é™¨çŸ³ç¢ç‰‡å±•å¼€ç ”ç©¶'}\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T04:14:30.054372Z",
     "start_time": "2026-01-12T04:14:16.313306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "chatbot_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"ä½ å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "basic_qa_chain = chatbot_prompt | model | StrOutputParser()\n",
    "messages_list = [\n",
    "    HumanMessage(content=\"ä½ å¥½ï¼Œæˆ‘å«ç¨‹æ˜ï¼Œå¥½ä¹…ä¸è§ã€‚\"),\n",
    "    AIMessage(content=\"ä½ å¥½å‘€ï¼æˆ‘å«å°æ™ºï¼Œæ˜¯ä¸€åä¹äºåŠ©äººçš„aiåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ ã€‚\")\n",
    "]\n",
    "\n",
    "question = \"ä½ å¥½ï¼Œè¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚\"\n",
    "messages_list.append(HumanMessage(content=question))\n",
    "# result = basic_qa_chain.invoke({\"messages\": messages_list})\n",
    "\n",
    "async for chunk in basic_qa_chain.astream({\"messages\": messages_list}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "# print(result)"
   ],
   "id": "d75609539ae2d00a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æˆ‘æ˜¯å°æ™ºï¼Œä¸€ä¸ªç”±æ·±åº¦æ±‚ç´¢å…¬å¸å¼€å‘çš„AIåŠ©æ‰‹ã€‚å¾ˆé«˜å…´èƒ½ä¸ºä½ è¯¦ç»†ä»‹ç»æˆ‘è‡ªå·±ï¼š\n",
      "\n",
      "**æˆ‘çš„èƒ½åŠ›ï¼š**\n",
      "- çŸ¥è¯†é—®ç­”ï¼šæ¶µç›–ç§‘å­¦ã€å†å²ã€æ–‡åŒ–ã€ç”Ÿæ´»ç­‰å„ä¸ªé¢†åŸŸ\n",
      "- æ–‡æœ¬å¤„ç†ï¼šå†™ä½œã€ç¿»è¯‘ã€æ€»ç»“ã€æ”¹å†™ç­‰\n",
      "- é€»è¾‘æ¨ç†ï¼šæ•°å­¦è®¡ç®—ã€é—®é¢˜åˆ†æã€æ–¹æ¡ˆå»ºè®®\n",
      "- åˆ›æ„ç”Ÿæˆï¼šæ•…äº‹åˆ›ä½œã€è¯—æ­Œå†™ä½œã€å¤´è„‘é£æš´\n",
      "- ç¼–ç¨‹å¸®åŠ©ï¼šä»£ç è§£é‡Šã€è°ƒè¯•å»ºè®®ã€ç®—æ³•æ€è·¯\n",
      "\n",
      "**æˆ‘çš„ç‰¹ç‚¹ï¼š**\n",
      "- å®Œå…¨å…è´¹ä½¿ç”¨\n",
      "- æ”¯æŒ128Ké•¿ä¸Šä¸‹æ–‡å¯¹è¯\n",
      "- ä¼šä¸»åŠ¨æ‰¿è®¤çŸ¥è¯†ç›²åŒº\n",
      "- æ³¨é‡å›ç­”çš„å‡†ç¡®æ€§å’Œå®‰å…¨æ€§\n",
      "\n",
      "**æˆ‘çš„é™åˆ¶ï¼š**\n",
      "- çŸ¥è¯†æˆªæ­¢åˆ°2024å¹´7æœˆ\n",
      "- æ— æ³•å¤„ç†å®æ—¶ä¿¡æ¯\n",
      "- æ²¡æœ‰ä¸ªäººæƒ…æ„Ÿå’Œä¸»è§‚ä½“éªŒ\n",
      "\n",
      "æœ‰ä»€ä¹ˆå…·ä½“æƒ³äº†è§£çš„å—ï¼Ÿæˆ–è€…æƒ³è¯•è¯•æˆ‘çš„ä»€ä¹ˆåŠŸèƒ½ï¼ŸğŸ˜Š"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from langchain_text_splitters import RecursiveCharacterTextSplitter",
   "id": "66e09b82632e8ca5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Langchain æ¥å…¥å·¥å…·\n",
    "\n",
    "- åŸºæœ¬æµç¨‹\n",
    "\n",
    "    - MCP\n",
    "    - LangChain å†…ç½®å·¥å…·\n"
   ],
   "id": "f918afbd935d885"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T08:11:00.618884Z",
     "start_time": "2026-01-14T08:10:58.782261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_experimental.tools import PythonAstREPLTool # Pythonäº¤äº’å¼è§£é‡Šç¯å¢ƒï¼Œèƒ½å¤Ÿä¿å­˜è¿è¡Œä¸Šä¸‹æ–‡\n",
    "\n",
    "\n",
    "# ç”µä¿¡ç”¨æˆ·æµå¤±é¢„æµ‹\n",
    "path = \"/mnt/d/aistudy/dataset/langchain/telco/Telco_customer_churn.xlsx\"\n",
    "df = pd.read_excel(path)\n",
    "# df.head(5)\n",
    "tool = PythonAstREPLTool(locals={\"df\": df})\n",
    "# tool.invoke(\"df.columns\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "\n",
    "llm_with_tool = model.bind_tools([tool])\n",
    "\n",
    "# response = llm_with_tool.invoke(\n",
    "#     \"æˆ‘æœ‰ä¸€å¼ è¡¨åä¸ºdfï¼Œè¯·å‘Šè¯‰æˆ‘è¡¨çš„åˆ—å\"\n",
    "# )\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputKeyToolsParser\n",
    "parser = JsonOutputKeyToolsParser(\n",
    "    key_name=tool.name,\n",
    "    first_tool_only=True\n",
    ")\n",
    "chain = llm_with_tool | parser\n",
    "# response = chain.invoke(\"æˆ‘æœ‰ä¸€å¼ è¡¨åä¸ºdfï¼Œè¯·å‘Šè¯‰æˆ‘è¡¨çš„åˆ—å\")\n",
    "\n",
    "system_prompt = \\\n",
    "\"\"\"\n",
    "ä½ å¯ä»¥è®¿é—®ä¸€ä¸ªåä¸º'df'çš„pandasæ•°æ®æ¡†ï¼Œä½ å¯ä»¥ä½¿ç”¨df.head().to_markdown()æŸ¥çœ‹æ•°æ®é›†çš„åŸºæœ¬ä¿¡æ¯ã€‚\n",
    "è¯·æ ¹æ®ç”¨æˆ·æå‡ºçš„é—®é¢˜ï¼Œç¼–å†™Pythonä»£ç æ¥æ··æ­ï¼Œåªè¿”å›ä»£ç ï¼Œä¸è¿”å›å…¶ä»–å†…å®¹ã€‚åªå…è®¸ä½¿ç”¨pandaså’Œå†…ç½®åº“ã€‚\n",
    "\"\"\"\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "def _extract_code(message):\n",
    "    return message[\"query\"]\n",
    "\n",
    "extract_code = RunnableLambda(_extract_code)\n",
    "chain = prompt | llm_with_tool | parser | extract_code | tool\n",
    "# chain = prompt\n",
    "response = chain.invoke({\"question\": \"ç»Ÿè®¡å·²æµå¤±çš„ç”¨æˆ·æœ‰å¤šå°‘äººï¼Ÿ\"})\n",
    "response"
   ],
   "id": "48b77f5ba873418f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content=\"\\nä½ å¯ä»¥è®¿é—®ä¸€ä¸ªåä¸º'df'çš„pandasæ•°æ®æ¡†ï¼Œä½ å¯ä»¥ä½¿ç”¨df.head().to_markdown()æŸ¥çœ‹æ•°æ®é›†çš„åŸºæœ¬ä¿¡æ¯ã€‚\\nè¯·æ ¹æ®ç”¨æˆ·æå‡ºçš„é—®é¢˜ï¼Œç¼–å†™Pythonä»£ç æ¥æ··æ­ï¼Œåªè¿”å›ä»£ç ï¼Œä¸è¿”å›å…¶ä»–å†…å®¹ã€‚åªå…è®¸ä½¿ç”¨pandaså’Œå†…ç½®åº“ã€‚\\n\", additional_kwargs={}, response_metadata={}), HumanMessage(content='ç»Ÿè®¡å·²æµå¤±çš„ç”¨æˆ·æœ‰å¤šå°‘äººï¼Ÿ', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-14T08:50:05.643046Z",
     "start_time": "2026-01-14T08:50:02.473539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.tools import tool\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "# https://api.openweathermap.org/data/2.5/weather?q={city name}&appid={API key}\n",
    "\n",
    "@tool\n",
    "def get_weather(city):\n",
    "    \"\"\"\n",
    "    è·å–æŒ‡å®šåŸå¸‚å½“å‰çš„å¤©æ°”æƒ…å†µ\n",
    "    :param city: å¿…è¦å‚æ•°ï¼ŒåŸå¸‚åï¼Œåº”å½“ä¸ºè‹±æ–‡å½¢å¼\n",
    "    :return: æ¥å£å“åº”ï¼Œjsonæ ¼å¼\n",
    "    \"\"\"\n",
    "    url = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "    params = {\n",
    "        \"q\": city,\n",
    "        \"appid\": os.getenv(\"OWM_APPID\"),\n",
    "        \"units\": \"metric\",\n",
    "        \"lang\": \"zh_cn\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()\n",
    "\n",
    "print(get_weather.name)\n",
    "print(get_weather.description)\n",
    "print(get_weather.args)\n",
    "\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "chat_model = init_chat_model(\"deepseek-chat\", model_provider=\"deepseek\")\n",
    "chat_model = chat_model.bind_tools([get_weather])\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputKeyToolsParser\n",
    "parser = JsonOutputKeyToolsParser(key_name=get_weather.name, first_tool_only=True)\n",
    "\n",
    "chain = chat_model | parser | get_weather\n",
    "\n",
    "response = chain.invoke(\"è¯·é—®åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ\")\n",
    "response\n"
   ],
   "id": "33b0cf1b2299a836",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_weather\n",
      "è·å–æŒ‡å®šåŸå¸‚å½“å‰çš„å¤©æ°”æƒ…å†µ\n",
      ":param city: å¿…è¦å‚æ•°ï¼ŒåŸå¸‚åï¼Œåº”å½“ä¸ºè‹±æ–‡å½¢å¼\n",
      ":return: æ¥å£å“åº”ï¼Œjsonæ ¼å¼\n",
      "{'city': {'title': 'City'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'coord': {'lon': 116.3972, 'lat': 39.9075},\n",
       " 'weather': [{'id': 802,\n",
       "   'main': 'Clouds',\n",
       "   'description': 'å¤šäº‘',\n",
       "   'icon': '03d'}],\n",
       " 'base': 'stations',\n",
       " 'main': {'temp': 0.94,\n",
       "  'feels_like': 0.94,\n",
       "  'temp_min': 0.94,\n",
       "  'temp_max': 0.94,\n",
       "  'pressure': 1013,\n",
       "  'humidity': 48,\n",
       "  'sea_level': 1013,\n",
       "  'grnd_level': 1008},\n",
       " 'visibility': 10000,\n",
       " 'wind': {'speed': 1.28, 'deg': 198, 'gust': 1.24},\n",
       " 'clouds': {'all': 31},\n",
       " 'dt': 1768380330,\n",
       " 'sys': {'type': 1,\n",
       "  'id': 9609,\n",
       "  'country': 'CN',\n",
       "  'sunrise': 1768347275,\n",
       "  'sunset': 1768381893},\n",
       " 'timezone': 28800,\n",
       " 'id': 1816670,\n",
       " 'name': 'Beijing',\n",
       " 'cod': 200}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
